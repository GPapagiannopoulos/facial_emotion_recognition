import os
import numpy as np 
import torch 
import torch.nn as nn 
import pandas as pd 
import matplotlib.pyplot as plt 
from tqdm import tqdm
from torchvision import datasets 
from torchvision import transforms
from torchvision.io import read_image
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torch.utils.data.sampler import SubsetRandomSampler 

#Configure device 
if torch.cuda.is_available():
    device = 'cuda'
else:
    device = 'cpu'

# Loading a custom dataset
class FacialExpressionDataset(Dataset):
    def __init__(self, annotations_file, img_dir, transform = None, target_transform = None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform
    
    def __len__(self):
        return len(self.img_labels)
    
    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = read_image(img_path).type(torch.float32)
        label = self.img_labels.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        
        return image, label

# Initialize the facial expression dataset
facial_expressions = FacialExpressionDataset(annotations_file = "C:/Users/George/OneDrive/Documents/Coding Projects/Datasets/emotion recognition/data/labels.csv",
                                             img_dir = "C:/Users/George/OneDrive/Documents/Coding Projects/Datasets/emotion recognition/data\AffectNet/")

# The calculate statistics function returns the average value for each channel across all pixels
def calculate_statistics():
    total_mean = torch.zeros(3)
    total_std = torch.zeros(3)
    for image in tqdm(facial_expressions):
        total_mean += torch.mean(image[0], dim=(1,2), dtype = torch.float32)
        total_std += torch.std(image[0].type(torch.float32), dim=(1,2))

    mean = total_mean/facial_expressions.__len__()
    std = total_std/facial_expressions.__len__()
    return mean, std


# The mean and std have already been calculated using the calulate_statistics() function
# mean, std = calculate_statistics()
mean = [136.0891/255.0, 110.6775/255.0,  98.6695/255.0]
std = [66.0843/255.0, 60.0459/255.0, 58.1655/255.0]

# Set Parameters 
batch_size = 128
random_seed = 42
training_size = 0.8
valid_size = 0.1
shuffle = True

# Define normalization parameters 
normalize = transforms.Normalize(mean= mean, std= std)

# Define the transformations
transform = transforms.Compose([
    transforms.Resize((224,224)),
    normalize
])
target_transform = transforms.Compose(
                    [lambda x:torch.LongTensor([x]), # or just torch.tensor
                     lambda x:torch.nn.functional.one_hot(x,8)])

# Define the split between the training and the validation dataset
num_train = facial_expressions.__len__()
indices = list(range(num_train))
split_one = int(np.floor(training_size * num_train))
split_two = split_one + int(np.floor(valid_size * num_train))

# Shuffle the dataset indices
np.random.seed(random_seed)
np.random.shuffle(indices)

# Splitting the training into training and validation using the shuffled indices
train_idx, valid_idx, test_idx = indices[:split_one], indices[split_one:split_two], indices[split_two:]

# Create a generator that randomly samples the two sets 
train_sampler = SubsetRandomSampler(train_idx)
valid_sampler = SubsetRandomSampler(valid_idx)
test_sampler = SubsetRandomSampler(test_idx)

# Initialize the facial expression dataset with the transformations
facial_expressions = FacialExpressionDataset(annotations_file = "C:/Users/George/OneDrive/Documents/Coding Projects/Datasets/emotion recognition/data/labels.csv",
                                             img_dir = "C:/Users/George/OneDrive/Documents/Coding Projects/Datasets/emotion recognition/data\AffectNet/",
                                             transform = transform, 
                                             target_transform = target_transform)

# Pass the dataset into the DataLoader so that it can be batched during training
train_loader = torch.utils.data.DataLoader(
      facial_expressions, batch_size=batch_size, sampler=train_sampler, pin_memory=True)
valid_loader = torch.utils.data.DataLoader(
      facial_expressions, batch_size=batch_size, sampler=valid_sampler, pin_memory=True)
test_loader = torch.utils.data.DataLoader(
    facial_expressions, batch_size=batch_size, sampler=test_sampler, pin_memory=True)

# Define the Residual Block which contains our skip connection 
class ResidualBlock(nn.Module):
    def __init__(self, input_channels, output_channels, stride = 1, downsample = None):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Sequential(
                     nn.Conv2d(input_channels, output_channels, kernel_size = 3, stride = stride, padding = 1),
                     nn.BatchNorm2d(output_channels),
                     nn.ReLU()
                     )
        self.conv2 = nn.Sequential(
                     nn.Conv2d(output_channels, output_channels, kernel_size = 3, stride = 1, padding = 1),
                     nn.BatchNorm2d(output_channels)
                     )
        self.downsample = downsample # a function to make sure the dimensions match so that the skip connection is possible 
        self.relu = nn.ReLU()
        self.output_channels = output_channels

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.conv2(out)
        if self.downsample:
            residual = self.downsample(x)
        out += residual 
        out = self.relu(out)

        return out 

class ResNet(nn.Module):
    def __init__(self, block, layers, num_classes = 8):
        super(ResNet, self).__init__()
        self.inplanes = 64
        self.conv1 = nn.Sequential(
                     nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3),
                     nn.BatchNorm2d(64),
                     nn.ReLU()
                     )
        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)
        self.layer0 = self.make_layer(block, 64, layers[0], stride = 1)
        self.layer1 = self.make_layer(block, 128, layers[1], stride = 2)
        self.layer2 = self.make_layer(block, 256, layers[2], stride = 2)
        self.layer3 = self.make_layer(block, 512, layers[3], stride = 2)
        self.avgpool = nn.AvgPool2d(7, stride = 1)
        self.fc = nn.Linear(512, num_classes)
    
    def make_layer(self, block, planes, blocks, stride = 1):
        downsample = None
        if stride != 1 or self.inplanes != planes:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes, kernel_size = 1, stride = stride),
                nn.BatchNorm2d(planes)
            )
        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes 
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))
        
        return nn.Sequential(*layers)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.maxpool(x)
        x = self.layer0(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)

        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        return x

# Defining hyperparameters
num_classes = 8
num_epochs = 20
learning_rate = 0.0005

model = ResNet(ResidualBlock, [3, 4, 6, 4]).to(device)

#Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  

#Train the model
import gc
total_step = len(train_loader)

torch.backends.cudnn.benchmark = True

for epoch in tqdm(range(num_epochs)):
    for (images, labels) in tqdm(train_loader):  
      #Move tensors to the configured device
      images = images.to(device)
      labels = labels.to(device)

      #Forward pass
      outputs = model(images)
      loss = criterion(outputs, labels)

      #Backward and optimize
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      del images, labels, outputs
      torch.cuda.empty_cache()
      gc.collect()

    print ('Epoch [{}/{}], Loss: {:.4f}' 
                 .format(epoch+1, num_epochs, loss.item()))

  #Validation
    with torch.no_grad():
      correct = 0
      total = 0
      for images, labels in tqdm(valid_loader):
          images = images.to(device)
          labels = labels.to(device)
          outputs = model(images)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum().item()
          del images, labels, outputs

      print('Accuracy of the network on the {} validation images: {:.3f} %'.format(3000, 100 * correct / total))

with torch.no_grad():
    model.eval()
    correct = 0
    total = 0
    for images, labels in tqdm(test_loader):
        images = images.to(device)
        labels = labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        del images, labels, outputs

    print('Accuracy of the network on the {} testing images: {:.3f} %'.format(3001, 100 * correct / total))

torch.save(model, "C:/Users/George/OneDrive/Documents/Coding Projects/MachineLearning/FER/Models/ResNetv1.0/model.pt")
torch.save(model.state_dict(), "C:/Users/George/OneDrive/Documents/Coding Projects/MachineLearning/FER/Models/ResNetv1.0/state_dict.pt")