import os
import numpy as np 
import torch 
import torch.nn as nn 
import pandas as pd
from torchsummary import summary
import matplotlib.pyplot as plt
from tqdm import tqdm
from torchvision.transforms import v2
from torchvision.io import read_image
from torch.utils.data import Dataset
from torch.utils.data.sampler import SubsetRandomSampler 
import torch.optim.lr_scheduler as lr_scheduler

#Configure device 
if torch.cuda.is_available():
    device = 'cuda'
else:
    device = 'cpu'

# Paths to model and dataset folders 
model_path = "C:/Users/George/OneDrive/Documents/Coding Projects/MachineLearning/FER/Models/ResNetv1.2/model.pt"
model_state_dict_path = "C:/Users/George/OneDrive/Documents/Coding Projects/MachineLearning/FER/Models/ResNetv1.2/state_dict.pt"
label_file = "C:/Users/George/OneDrive/Documents/Coding Projects/Datasets/emotion recognition/data/labels.csv"
image_dir = "C:/Users/George/OneDrive/Documents/Coding Projects/Datasets/emotion recognition/data/AffectNet/"


# Loading a custom dataset
class FacialExpressionDataset(Dataset):
    def __init__(self, annotations_file, img_dir, transform = None, target_transform = None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform
    
    def __len__(self):
        return len(self.img_labels)
    
    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = read_image(img_path).type(torch.float32)
        label = self.img_labels.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        
        return image, label

# Initialize the facial expression dataset
facial_expressions = FacialExpressionDataset(annotations_file = label_file,
                                             img_dir = image_dir)

# The calculate statistics function returns the average value for each channel across all pixels
def calculate_statistics():
    total_mean = torch.zeros(3)
    total_std = torch.zeros(3)
    for image in tqdm(facial_expressions):
        total_mean += torch.mean(image[0], dim=(1,2), dtype = torch.float32)
        total_std += torch.std(image[0].type(torch.float32), dim=(1,2))

    mean = total_mean/facial_expressions.__len__()
    std = total_std/facial_expressions.__len__()
    return mean, std


# The mean and std have already been calculated using the calulate_statistics() function
# mean, std = calculate_statistics()
mean = [136.0891, 110.6775,  98.6695]
std = [66.0843, 60.0459, 58.1655]

# Set Parameters 
batch_size = 64
random_seed = 42
training_size = 0.8
valid_size = 0.1
shuffle = True

# Define normalization parameters 
normalize = v2.Normalize(mean= mean, std= std)

# Define the transformations
transform = v2.Compose([
    v2.RandomHorizontalFlip(0.4),
    v2.Resize((224,224)),
    normalize
])
target_transform = v2.Compose(
                    [lambda x:torch.LongTensor([x]), # or just torch.tensor
                     lambda x:torch.nn.functional.one_hot(x,8)])

# Define the split between the training and the validation dataset
num_train = facial_expressions.__len__()
indices = list(range(num_train))
split_one = int(np.floor(training_size * num_train))
split_two = split_one + int(np.floor(valid_size * num_train))

# Shuffle the dataset indices
np.random.seed(random_seed)
np.random.shuffle(indices)

# Splitting the training into training and validation using the shuffled indices
train_idx, valid_idx, test_idx = indices[:split_one], indices[split_one:split_two], indices[split_two:]

# Create a generator that randomly samples the two sets 
train_sampler = SubsetRandomSampler(train_idx)
valid_sampler = SubsetRandomSampler(valid_idx)
test_sampler = SubsetRandomSampler(test_idx)

# Initialize the facial expression dataset with the transformations
facial_expressions = FacialExpressionDataset(annotations_file = label_file,
                                             img_dir = image_dir,
                                             transform = transform, 
                                             target_transform = target_transform)

# Pass the dataset into the DataLoader so that it can be batched during training
train_loader = torch.utils.data.DataLoader(
      facial_expressions, batch_size=batch_size, sampler=train_sampler, pin_memory=True)
valid_loader = torch.utils.data.DataLoader(
      facial_expressions, batch_size=batch_size, sampler=valid_sampler, pin_memory=True)
test_loader = torch.utils.data.DataLoader(
    facial_expressions, batch_size=batch_size, sampler=test_sampler, pin_memory=True)

# Define the Residual Block which contains our skip connection 
class ResidualBlock(nn.Module):
    def __init__(self, input_channels, output_channels, stride = 1, downsample = None):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Sequential(
                     nn.Conv2d(input_channels, output_channels, kernel_size = 3, stride = stride, padding = 1),
                     nn.BatchNorm2d(output_channels),
                     nn.ReLU()
                     )
        self.conv2 = nn.Sequential(
                     nn.Conv2d(output_channels, output_channels, kernel_size = 3, stride = 1, padding = 1),
                     nn.BatchNorm2d(output_channels)
                     )
        self.downsample = downsample # a function to make sure the dimensions match so that the skip connection is possible 
        self.relu = nn.ReLU()
        self.output_channels = output_channels

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.conv2(out)
        if self.downsample:
            residual = self.downsample(x)
        out += residual 
        out = self.relu(out)

        return out 

class ResNet(nn.Module):
    def __init__(self, block, layers, num_classes = 8):
        super(ResNet, self).__init__()
        self.inplanes = 64
        self.conv1 = nn.Sequential(
                     nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3),
                     nn.BatchNorm2d(64),
                     nn.ReLU()
                     )
        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)
        self.layer0 = self.make_layer(block, 64, layers[0], stride = 1)
        self.layer1 = self.make_layer(block, 128, layers[1], stride = 2)
        self.layer2 = self.make_layer(block, 256, layers[2], stride = 2)
        self.layer3 = self.make_layer(block, 512, layers[3], stride = 2)
        self.avgpool = nn.AvgPool2d(7, stride = 1)
        self.fc = nn.Linear(512, num_classes)
    
    def make_layer(self, block, planes, blocks, stride = 1):
        downsample = None
        if stride != 1 or self.inplanes != planes:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes, kernel_size = 1, stride = stride),
                nn.BatchNorm2d(planes)
            )
        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes 
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))
        
        return nn.Sequential(*layers)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.maxpool(x)
        x = self.layer0(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)

        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        return x

# Defining hyperparameters
num_classes = 8
num_epochs = 25
learning_rate = 0.1

# Instantiate model 
model = ResNet(ResidualBlock, [3, 4, 6, 4]).to(device)

#Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)  

# Define learning rate schedule 
def custom_lr(epoch):
    base_lr = 0.1
    factor = 0.01
    return base_lr/(1+factor*epoch)
scheduler = lr_scheduler.LambdaLR(optimizer, custom_lr)

#Train the model
import gc
total_step = len(train_loader)

torch.backends.cudnn.benchmark = True
trainingEpochLoss = []
validationEpochLoss = []

print(summary(model.cuda(), (3, 224, 224)))

for epoch in tqdm(range(num_epochs)):
    training_loss = []
    for (images, labels) in tqdm(train_loader):  
      #Move tensors to the configured device
      images = images.to(device)
      labels = labels.to(device)

      #Forward pass
      outputs = model(images)
      loss = criterion(outputs, labels)
      training_loss.append(loss.item())
      #Backward and optimize
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      del images, labels, outputs
      torch.cuda.empty_cache()
      gc.collect()

    print ('Epoch [{}/{}], Loss: {:.4f}' 
                 .format(epoch+1, num_epochs, loss.item()))
    trainingEpochLoss.append(np.array(training_loss).mean())

  #Validation
    with torch.no_grad():
      validation_loss = []
      correct = 0
      total = 0
      for images, labels in tqdm(valid_loader):
          images = images.to(device)
          labels = labels.to(device)
          outputs = model(images)
          val_loss = criterion(outputs, labels)
          validation_loss.append(val_loss.item())
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum().item()
          del images, labels, outputs

      print('Accuracy: {:.3f} %, Loss: {:.3f}'.format(100 * correct / total, val_loss.item()))
    validationEpochLoss.append(np.array(validation_loss).mean())

   # Checkpoint the model 
    torch.save(model, model_path)
    torch.save(model.state_dict(), model_state_dict_path)

with torch.no_grad():
    model.eval()
    correct = 0
    total = 0
    for images, labels in tqdm(test_loader):
        images = images.to(device)
        labels = labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        del images, labels, outputs

    print('Accuracy of the network on the {} testing images: {:.3f} %'.format(3001, 100 * correct / total))

for name, param in model.named_parameters():
    if param.requires_grad:
        print (name, param.data)

plt.plot(trainingEpochLoss, label='train_loss')
plt.plot(validationEpochLoss,label='val_loss')
plt.legend()
plt.savefig("C:/Users/George/OneDrive/Documents/Coding Projects/MachineLearning/FER/Models/ResNetv1.2/loss")
plt.show()